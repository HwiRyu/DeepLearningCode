{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 필요한 라이브러리\n",
    "\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "from scipy.special import expit\n",
    "import os\n",
    "\n",
    "\n",
    "#출력 이쁘게 해주는 함수\n",
    "\n",
    "def print_over(text):\n",
    "    sys.stdout.write('\\r' + text)\n",
    "    sys.stdout.flush()\n",
    "def generate_formatted_string(number):\n",
    "    # 입력된 숫자를 정수와 소수 부분으로 분리합니다.\n",
    " \n",
    "    number = f\"{number:.14f}\"\n",
    "\n",
    "    int_part, decimal_part = str(number).split('.')\n",
    "    decimal_part = decimal_part[:13]\n",
    "    if len(int_part) > 3:\n",
    "        format_string = 'OUT!OFVALUEPLEASE'\n",
    "    # 정수 부분을 1000자리로 맞추기 위해 필요한 '0'의 개수를 계산합니다.\n",
    "    else:\n",
    "        num_zeros_needed = max(0, 3 - len(int_part))\n",
    "        int_part = int_part[-3:]\n",
    "        # 1000자리의 '0'과 정수 부분, 소수 부분을 합쳐서 문자열을 생성합니다.\n",
    "        format_string = '0' * num_zeros_needed + int_part + '.' + decimal_part.ljust(13, '0')\n",
    "\n",
    "    return format_string\n",
    "    \n",
    "# 필요한 함수 정의\n",
    "\n",
    "# 활성 함수\n",
    "\n",
    "#렐루\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "def relu_derivative(x):\n",
    "    return np.where(np.array(x) > 0, 1, 0)\n",
    "\n",
    "#리키렐루\n",
    "def leaky_relu(x, alpha=0.001):\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.001):\n",
    "    return np.where(np.array(x) > 0, 1, alpha)\n",
    "\n",
    "#시그모이드\n",
    "def sigmoid(x):\n",
    "    return 1000 / (1000 * (1 + np.exp(-x)))\n",
    "def sigmoid_derivative(x):\n",
    "    return 1 / (1 + np.exp(-x)) * (1 - 1 / (1 + np.exp(-x)))\n",
    "\n",
    "#항등함수\n",
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "def identity_function_derivative(x):\n",
    "    return np.ones(x.shape)\n",
    "\n",
    "#소프트맥스 함수\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # overflow 방지\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    softmax_x = softmax(x)\n",
    "    return softmax_x * (1 - softmax_x)\n",
    "\n",
    "\n",
    "#활성함수[0] = 원함수\n",
    "#활성함수[1] = 도함수\n",
    "\n",
    "sig = sigmoid, sigmoid_derivative\n",
    "ralo = relu, relu_derivative\n",
    "leaky_ralo = leaky_relu, leaky_relu_derivative\n",
    "identity = identity_function, identity_function_derivative\n",
    "soft = softmax, softmax_derivative\n",
    "\n",
    "\n",
    "# 로스 함수\n",
    "\n",
    "#MSE\n",
    "def mean_squared_error(output, target):\n",
    "    return np.mean((output - target) ** 2)\n",
    "def mse_derivative(output, target):\n",
    "    return 2 * (output - target)\n",
    "\n",
    "#BSE\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "mse = mean_squared_error, mse_derivative\n",
    "\n",
    "\n",
    "# 신경망 초기화때 사용할 분포들\n",
    "\n",
    "def gaussian_distribution(input1, input2, multiple, plus):\n",
    "    return np.random.randn(input1, input2) * multiple + plus\n",
    "\n",
    "def zero_distribution(input1, input2):\n",
    "    return np.zeros((input1, input2))\n",
    "\n",
    "def one_distribution(input1, input2, multiple):\n",
    "    return np.ones((input1, input2)) * multiple\n",
    "\n",
    "# 오차역전파에서 쓸 함수\n",
    "\n",
    "def gradient_desent(learning_rate, gradient):        \n",
    "    return learning_rate * gradient\n",
    "\n",
    "def matrix_product_PI(function1, function2, f, K):\n",
    "    if K == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        x = (function1[f - 1]).T * np.array(function2[f -1])\n",
    "        for k in range(K-1):\n",
    "            y = (function1[f - (k + 2)]).T * np.array(function2[f - (k+2)])\n",
    "            x = x @ y\n",
    "            \n",
    "    return x\n",
    "\n",
    "def grape_show(x_values, y_values_funk, y_values_square):\n",
    "    plt.plot(x_values, y_values_funk, label='Funk Object')\n",
    "    plt.plot(x_values, y_values_square, label='want_funtion', linestyle='--', color='red')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Funk Object vs. want_funtion')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim(-1.1, 1.1)  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mnist 데이터셋에서 임의로 Parameter1 만큼의 갯수만 가져오기\n",
    "\n",
    "def get_random_png_files(folder_path, num_files):\n",
    "    png_files = glob.glob(os.path.join(folder_path, \"*.png\"))\n",
    "    random_files = random.sample(png_files, num_files)\n",
    "    return random_files\n",
    "\n",
    "folder_path6 = \"C:/Users/Persist/Downloads/mnist_png.tar/mnist_png/training/6\"  \n",
    "random_png_files6 = get_random_png_files(folder_path6, 5000)\n",
    "\n",
    "folder_path3 = \"C:/Users/Persist/Downloads/mnist_png.tar/mnist_png/training/3\"  \n",
    "random_png_files3 = get_random_png_files(folder_path3, 5000)\n",
    "\n",
    "folder_path_testing_3 = \"C:/Users/Persist/Downloads/mnist_png.tar/mnist_png/testing/3\"  \n",
    "random_png_files_test_3 = get_random_png_files(folder_path_testing_3, 500)\n",
    "\n",
    "\n",
    "ranchooseimage6 = random.sample(random_png_files6, 1)\n",
    "ranchooseimage3 = random.sample(random_png_files3, 1)\n",
    "ranchooseimage_test_3 = random.sample(random_png_files_test_3, 1)\n",
    "\n",
    "#print(ranchooseimage)\n",
    "\n",
    "#이미지를 열고 행렬로 변환\n",
    "\n",
    "def process_image(image_path):\n",
    "    # 이미지 열기\n",
    "    with Image.open(image_path) as image:\n",
    "        # 이미지의 해상도 (너비, 높이) 저장\n",
    "        width, height = image.size\n",
    "        Weight = width\n",
    "        Height = height\n",
    "        \n",
    "        # 이미지를 그레이스케일로 변환\n",
    "        gray_image = image.convert(\"L\")\n",
    "        \n",
    "        # 그레이스케일 이미지를 numpy 배열로 변환\n",
    "        gray_array = np.array(gray_image).reshape(1, -1)\n",
    "        \n",
    "        return Height, Weight, gray_array\n",
    "    \n",
    "DataSet6 = []\n",
    "DataSet3 = []\n",
    "DataSet_test_3 = []\n",
    "\n",
    "for i in range(5000):\n",
    "    random_image_path6 = random_png_files6[i]\n",
    "    _, _, gray_array = process_image(random_image_path6)\n",
    "    normalized_array = (gray_array / 255.0)  # 255로 나누어 정규화\n",
    "    DataSet6.append(normalized_array)\n",
    "    \n",
    "for i in range(5000):\n",
    "    random_image_path3 = random_png_files3[i]\n",
    "    _, _, gray_array = process_image(random_image_path3)\n",
    "    normalized_array = (gray_array / 255.0)  # 255로 나누어 정규화\n",
    "    DataSet3.append(normalized_array)\n",
    "    \n",
    "    \n",
    "for i in range(500):\n",
    "    random_image_path_test3 = random_png_files_test_3[i]\n",
    "    _, _, gray_array = process_image(random_image_path_test3)\n",
    "    normalized_array = (gray_array / 255.0)  # 255로 나누어 정규화\n",
    "    DataSet_test_3.append(normalized_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#신경망\n",
    "#신경망 = Neurel_Network() 로 선언\n",
    "#신경망([p0,p1,p2,...,pk+1], weight 초깃값, bias 초깃값, 은닉층 활성함수, 출력 활성함수)\n",
    "\n",
    "\n",
    "class Neurel_Network:\n",
    "    def __init__(self):\n",
    "        self.layer_number = 0\n",
    "        self.weight = []\n",
    "        self.bias = []\n",
    "        self.layer = []\n",
    "        self.after_layer = []\n",
    "        self.external_active_function = []\n",
    "        self.external_last_active_function = []\n",
    "        self.final_layer = []\n",
    "            \n",
    "    def active_function(self, input, i): # i = 0이면 원래 함수, i = 1이면 미분된 함수\n",
    "        function = self.external_active_function[i]\n",
    "        return function(input)\n",
    "    \n",
    "    def last_active_function(self, input, i):\n",
    "        function = self.external_last_active_function[i]\n",
    "        return function(input)\n",
    "\n",
    "    def initialize_network(self, perceptron, weight_initial, bias_initial, function, last_function):\n",
    "        self.layer_number = len(perceptron)\n",
    "        self.external_active_function = function\n",
    "        self.external_last_active_function = last_function\n",
    "        \n",
    "        for i in range((self.layer_number) - 1):\n",
    "            self.weight.append(weight_initial(perceptron[i], perceptron[i + 1], 0.1, 0))\n",
    "            self.bias.append(bias_initial(1, perceptron[i + 1]))\n",
    "\n",
    "    def forward(self, input):\n",
    "        #after_layer[k] = Active_function(L[k])\n",
    "        \n",
    "        self.layer = []\n",
    "        self.after_layer = []\n",
    "        self.final_layer = []\n",
    "        self.after_layer.append(np.array(input).reshape(1, -1))\n",
    "        \n",
    "        self.layer.append(np.array(input).reshape(1,-1)) #L[0] = input\n",
    "        #L[1]~L[k]\n",
    "        for i in range((self.layer_number) - 2):\n",
    "            #i = 0 일떄, l[1] = layer[0] * w[0]\n",
    "            self.layer.append(np.array((self.after_layer[i] @ self.weight[i]) + self.bias[i]))\n",
    "            self.after_layer.append(np.array(self.active_function(self.layer[i+1], 0)))\n",
    "            \n",
    "        self.layer.append(np.array((self.layer[(self.layer_number) - 2] @ self.weight[(self.layer_number) - 2]) + self.bias[(self.layer_number) - 2]))\n",
    "        self.after_layer.append(np.array(self.last_active_function(self.layer[(self.layer_number) - 1], 0)))\n",
    "        output = self.after_layer[(self.layer_number)-1]\n",
    "        \n",
    "        #Final_layer\n",
    "        for i in range(len(self.layer) - 1):\n",
    "            self.final_layer.append(np.array(self.active_function(self.layer[i], 1)))\n",
    "        \n",
    "        self.final_layer.append(np.array(self.last_active_function(self.layer[len(self.layer) - 1], 1)))\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#오차역전파\n",
    "def backward(neural_network, Loss_function, inputdataset, targetdataset, learning_rate):\n",
    "\n",
    "        \n",
    "    minibatch = len(inputdataset)\n",
    "        \n",
    "\n",
    "    #초기화\n",
    "    gradient_weight = [0]*((neural_network.layer_number)-1)\n",
    "    gradient_bias= [0]*((neural_network.layer_number)-1)\n",
    "    \n",
    "    weight = neural_network.weight\n",
    "    bias = neural_network.bias\n",
    "    \n",
    "    \n",
    "    for iteration_minibatch in range(minibatch):\n",
    "        \n",
    "        \n",
    "        input = inputdataset[iteration_minibatch]\n",
    "        target_data = targetdataset[iteration_minibatch]\n",
    "        #오차역전파 시작\n",
    "        output = neural_network.forward(input)\n",
    "        \n",
    "        #weight[k], bias[k]를 갱신함\n",
    "        gradient_weight[neural_network.layer_number-2] =\\\n",
    "        neural_network.after_layer[neural_network.layer_number-2].T\\\n",
    "        @Loss_function[1](output, target_data)\\\n",
    "        *neural_network.final_layer[neural_network.layer_number - 1]\n",
    "        \n",
    "        gradient_bias[neural_network.layer_number-2]=\\\n",
    "        Loss_function[1](output, target_data)\\\n",
    "        *neural_network.final_layer[neural_network.layer_number - 1]\n",
    "\n",
    "        weight[neural_network.layer_number-2] -= learning_rate * gradient_weight[neural_network.layer_number-2]\n",
    "        bias[neural_network.layer_number-2] -= learning_rate * gradient_bias[neural_network.layer_number-2]\n",
    "\n",
    "        #이후부터 하나씩 갱신해감\n",
    "\n",
    "        for i in range(1,(neural_network.layer_number)-1):\n",
    "            #output = neural_network.forward(input)\n",
    "                \n",
    "            gradient_weight[neural_network.layer_number - 2-i]=\\\n",
    "            neural_network.after_layer[neural_network.layer_number-2 -i].T\\\n",
    "            @(Loss_function[1](output, target_data)\\\n",
    "            *neural_network.final_layer[neural_network.layer_number - 1])\\\n",
    "            @matrix_product_PI(neural_network.weight, neural_network.final_layer, neural_network.layer_number-1, i)\n",
    "            \n",
    "            gradient_bias[neural_network.layer_number - 2-i]=\\\n",
    "            Loss_function[1](output, target_data)\\\n",
    "            *neural_network.final_layer[neural_network.layer_number - 1]\\\n",
    "            @matrix_product_PI(neural_network.weight, neural_network.final_layer, neural_network.layer_number-1, i)\n",
    "            \n",
    "            weight[neural_network.layer_number - 2-i] -= learning_rate * gradient_weight[neural_network.layer_number - 2-i] / minibatch\n",
    "            bias[neural_network.layer_number - 2-i] -= learning_rate * gradient_bias[neural_network.layer_number - 2-i] / minibatch\n",
    "        \n",
    "    neural_network.weight = weight\n",
    "    neural_network.bias = bias\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "    return Loss_function[0](input, target_data)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = Neurel_Network()\n",
    "\n",
    "N.initialize_network([784, 1024, 1024,512, 1], gaussian_distribution, zero_distribution, leaky_ralo, sig)\n",
    "\n",
    "len_data = len(DataSet3)\n",
    "\n",
    "loss_values = []\n",
    "wantrange = 100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data divide to MiniBatch\n",
    "def minibatch_learning(dataset, batch_size, type):\n",
    "    dataset_len = len(dataset)\n",
    "    minibatchset = []\n",
    "    if dataset_len % batch_size == 0:\n",
    "        batch_number = dataset_len // batch_size\n",
    "        last_iteration = 0\n",
    "    else:\n",
    "        batch_number = dataset_len // batch_size + 1\n",
    "        last_iteration = dataset_len % batch_size\n",
    "    \n",
    "   # if type == 'suffle':\n",
    "        \n",
    "    if type == 'continue':\n",
    "        if last_iteration == 0:\n",
    "            for i in range(batch_number):\n",
    "                A = []\n",
    "                for k in range(batch_size):\n",
    "                 A.append(dataset[i*batch_size + k])\n",
    "                minibatchset.append(A)\n",
    "    \n",
    "    return minibatchset\n",
    "    \n",
    "\n",
    "\n",
    "    # if len(inputdataset) % batchsize == 0:\n",
    "    #     batch_number = len(inputdataset) // batchsize\n",
    "    #     last_iteration = 0\n",
    "    #     minibatch = batchsize\n",
    "    # else:\n",
    "    #     batch_number = len(inputdataset) // batchsize + 1\n",
    "    #     last_iteration = len(inputdataset) % batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#def backward(neural_network, Loss_function, inputdataset, targetdataset, batchsize, learning_rate):\n",
    "\n",
    "real_data = [1] * 5000\n",
    "fake_data = [0] * 5000\n",
    "\n",
    "\n",
    "DataSetSuffle = DataSet3 + DataSet6\n",
    "CorrcetSetSuffle =  fake_data + real_data\n",
    "\n",
    "\n",
    "\n",
    "Datacombined = list(zip(DataSetSuffle, CorrcetSetSuffle))\n",
    "random.shuffle(Datacombined)\n",
    "DataSetSuffle, CorrcetSetSuffle = zip(*Datacombined)\n",
    "\n",
    "Inputdataset = minibatch_learning(DataSetSuffle, 10, 'continue')\n",
    "CorrectDataset = minibatch_learning(CorrcetSetSuffle, 10, 'continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 / 100000 | Loss: 000.1029225023342 | Output(6): [[2.45513949e-05]]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m what \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice([\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m])\n\u001b[0;32m      5\u001b[0m k \u001b[39m=\u001b[39m i \u001b[39m%\u001b[39m \u001b[39m5000\u001b[39m\n\u001b[1;32m----> 6\u001b[0m loss \u001b[39m=\u001b[39m backward(N, mse, Inputdataset[k], CorrectDataset[k], lr)\n\u001b[0;32m      7\u001b[0m loss_values\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m      8\u001b[0m output \u001b[39m=\u001b[39m N\u001b[39m.\u001b[39mforward(DataSet6[k])\n",
      "Cell \u001b[1;32mIn[4], line 46\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(neural_network, Loss_function, inputdataset, targetdataset, learning_rate)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39m#이후부터 하나씩 갱신해감\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,(neural_network\u001b[39m.\u001b[39mlayer_number)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     40\u001b[0m     \u001b[39m#output = neural_network.forward(input)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     gradient_weight[neural_network\u001b[39m.\u001b[39mlayer_number \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m\u001b[39m-\u001b[39mi]\u001b[39m=\u001b[39m\\\n\u001b[0;32m     43\u001b[0m     neural_network\u001b[39m.\u001b[39mafter_layer[neural_network\u001b[39m.\u001b[39mlayer_number\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m \u001b[39m-\u001b[39mi]\u001b[39m.\u001b[39mT\\\n\u001b[0;32m     44\u001b[0m     \u001b[39m@\u001b[39m(Loss_function[\u001b[39m1\u001b[39m](output, target_data)\\\n\u001b[0;32m     45\u001b[0m     \u001b[39m*\u001b[39mneural_network\u001b[39m.\u001b[39mfinal_layer[neural_network\u001b[39m.\u001b[39mlayer_number \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m])\\\n\u001b[1;32m---> 46\u001b[0m     \u001b[39m@matrix_product_PI\u001b[39m(neural_network\u001b[39m.\u001b[39;49mweight, neural_network\u001b[39m.\u001b[39;49mfinal_layer, neural_network\u001b[39m.\u001b[39;49mlayer_number\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, i)\n\u001b[0;32m     48\u001b[0m     gradient_bias[neural_network\u001b[39m.\u001b[39mlayer_number \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m\u001b[39m-\u001b[39mi]\u001b[39m=\u001b[39m\\\n\u001b[0;32m     49\u001b[0m     Loss_function[\u001b[39m1\u001b[39m](output, target_data)\\\n\u001b[0;32m     50\u001b[0m     \u001b[39m*\u001b[39mneural_network\u001b[39m.\u001b[39mfinal_layer[neural_network\u001b[39m.\u001b[39mlayer_number \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\\\n\u001b[0;32m     51\u001b[0m     \u001b[39m@matrix_product_PI\u001b[39m(neural_network\u001b[39m.\u001b[39mweight, neural_network\u001b[39m.\u001b[39mfinal_layer, neural_network\u001b[39m.\u001b[39mlayer_number\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, i)\n\u001b[0;32m     53\u001b[0m     weight[neural_network\u001b[39m.\u001b[39mlayer_number \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m\u001b[39m-\u001b[39mi] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m gradient_weight[neural_network\u001b[39m.\u001b[39mlayer_number \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m\u001b[39m-\u001b[39mi] \u001b[39m/\u001b[39m minibatch\n",
      "Cell \u001b[1;32mIn[1], line 130\u001b[0m, in \u001b[0;36mmatrix_product_PI\u001b[1;34m(function1, function2, f, K)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(K\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    129\u001b[0m         y \u001b[39m=\u001b[39m (function1[f \u001b[39m-\u001b[39m (k \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m)])\u001b[39m.\u001b[39mT \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39marray(function2[f \u001b[39m-\u001b[39m (k\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m)])\n\u001b[1;32m--> 130\u001b[0m         x \u001b[39m=\u001b[39m x \u001b[39m@\u001b[39m y\n\u001b[0;32m    132\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(wantrange):\n",
    "    \n",
    "    lr = 0.0001\n",
    "    what = np.random.choice([0,1])\n",
    "    k = i % 5000\n",
    "    loss = backward(N, mse, Inputdataset[k], CorrectDataset[k], lr)\n",
    "    loss_values.append(loss)\n",
    "    output = N.forward(DataSet6[k])\n",
    "    loss = generate_formatted_string(loss)\n",
    "    print_over(f\"Epoch: {i} / {wantrange} | Loss: {loss} | Output(6): {output}\")        \n",
    "\n",
    "\n",
    "        \n",
    "    # else:          \n",
    "    #     k = np.random.choice(len(DataSet3_mini) -1)\n",
    "\n",
    "\n",
    "    #     # k = np.random.choice(np.arange(0, 5000))\n",
    "\n",
    "    #     target = [0]*len_data\n",
    "    #     loss = backward(N, mse, DataSet3_mini[k], target,  lr)\n",
    "    #     loss_values.append(loss)\n",
    "    #     output = N.forward(DataSet3[k])\n",
    "    #     loss = generate_formatted_string(loss)\n",
    "    #     #output = generate_formatted_string(output)\n",
    "    #     print_over(f\"Epoch: {i} / {wantrange} | Loss: {loss} | Output(3): {output}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for k in range(100):\n",
    "#     print(N.forward(DataSet3[k]))\n",
    "\n",
    "# print('-----------------------------------------------')\n",
    "# for k in range(100):\n",
    "    \n",
    "#     print(N.forward(DataSet6[k]))\n",
    "    \n",
    "\n",
    "real = 0\n",
    "fake = 0\n",
    "non = 0\n",
    "\n",
    "can = 10000\n",
    "for i in range(can):\n",
    "\n",
    "    k = np.random.choice(np.arange(1, 500))\n",
    "    result = N.forward(DataSet_test_3[k])\n",
    "    # result = N.forward(DataSet6[k])\n",
    "    if result < 0.11:\n",
    "        real += 1\n",
    "    elif  result > 0.89:\n",
    "        fake += 1\n",
    "    else:\n",
    "        non += 1\n",
    "\n",
    "print(real, \"/\", can, non)\n",
    "\n",
    "#대충 15초 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 그래프 그리기\n",
    "plt.plot(range(len(loss_values)), loss_values, label='Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss vs. Iteration')\n",
    "plt.ylim(0, 10000)\n",
    "plt.show()\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.plot(range(len(loss_values)), loss_values, label='Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss vs. Iteration')\n",
    "plt.ylim(0, 10)\n",
    "plt.show()\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.plot(range(len(loss_values)), loss_values, label='Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss vs. Iteration')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.plot(range(len(loss_values)), loss_values, label='Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss vs. Iteration')\n",
    "plt.ylim(0, 0.1)\n",
    "plt.show()\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.plot(range(len(loss_values)), loss_values, label='Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss vs. Iteration')\n",
    "plt.ylim(0, 0.0001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
